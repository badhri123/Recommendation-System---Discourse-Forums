# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dqmqVcP1A1FRGB8UAh_8i0WCqHxHA4F8
"""

import requests
import json
import pprint
import pickle
from bs4 import BeautifulSoup

def cleans_dictionary(raw_dictionary):
    #get all the slugs and topic_id's out of this dictionary (make sure the parameters are string)
    #and make a new dictionary with the 30 topics just the id's and slugs
    #new_submissions = [{'topic_id': 23124,'slug':'debit-cards'},{'topic_id': 846283,'slug':'credit-card'}]
    topics = raw_dictionary['topic_list']['topics']
    list_of_topics = []
    for i in topics:
        list_of_topics.append([i['id'],i['slug']])
    return list_of_topics



i = 1

# Enter desired URL in user_url ( In the below format )
# Go the webpage you want to crawl, inpsect the network on the page and get the Request URL.
user_url = "https://forum.gethopscotch.com/latest.json?no_definitions=true&order=default&page="



new_submissions = []

latest_url = "https://forum.gethopscotch.com/latest"
headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36'}
first_response = requests.get(latest_url).text
soup = BeautifulSoup(first_response, 'lxml')
topic_list = soup.find_all('span', class_='link-top-line')
for topic in topic_list:
    new_submissions.append([topic.find('a').attrs['href'].split("/")[-1], topic.get_text().strip()])

crawler_condition = True
while crawler_condition == True:
    url = user_url+str(i)
    r = (requests.get(url)).text
    raw_dictionary = json.loads(r)
    if len(raw_dictionary['topic_list']['topics']) <= 0:
        crawler_condition = False
        break
    else:
        clean_dictionary = cleans_dictionary(raw_dictionary)
        new_submissions.extend(clean_dictionary)
        i += 1


print("Number of Topics in the website is",len(new_submissions))

print("Saving the topic names as pickle file")

with open("all_the_topic_names.pickle", "wb") as output_file:
     pickle.dump(new_submissions, output_file)